{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\IRONMAN\\10\\10x\\rossman_predictive_analysis\\notebooks\\deep_learning_modeling.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/IRONMAN/10/10x/rossman_predictive_analysis/notebooks/deep_learning_modeling.ipynb#ch0000000?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstattools\u001b[39;00m \u001b[39mimport\u001b[39;00m adfuller, acf, pacf\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/IRONMAN/10/10x/rossman_predictive_analysis/notebooks/deep_learning_modeling.ipynb#ch0000000?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/IRONMAN/10/10x/rossman_predictive_analysis/notebooks/deep_learning_modeling.ipynb#ch0000000?line=21'>22</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/IRONMAN/10/10x/rossman_predictive_analysis/notebooks/deep_learning_modeling.ipynb#ch0000000?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/IRONMAN/10/10x/rossman_predictive_analysis/notebooks/deep_learning_modeling.ipynb#ch0000000?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import ticker\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing warning messages\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../scripts')\n",
    "\n",
    "from data_viz import Data_Viz\n",
    "from data_cleaning import DataCleaner\n",
    "from data_transformation import DataTransformer\n",
    "\n",
    "DV = Data_Viz(\"../logs/deep_model_notebook.log\")\n",
    "DC = DataCleaner(\"../logs/deep_model_notebook.log\")\n",
    "DT = DataTransformer(\"../logs/deep_model_notebook.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "\n",
    "train = pd.read_csv(\"../data/train_store.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking available variables\n",
    "train = train.sort_values(by=\"Date\")\n",
    "train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "\n",
    "print(\"rows and columsn: \", train.shape)\n",
    "DV.summ_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales per Day\n",
    "\n",
    "tsa_df_days = train[[\"Date\", \"Sales\"]].groupby([\"Date\"]).agg({\"Sales\":\"mean\"})\n",
    "\n",
    "print(\"rows and columns: \", tsa_df_days.shape)\n",
    "print(DV.summ_columns(tsa_df_days))\n",
    "tsa_df_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV.plot_line(tsa_df_days, \"Date\", \"Sales\", [15, 5], \"sales per day\", \"sales_day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating week based data\n",
    "tsa_df_weeks = train[[\"WeekOfYear\", \"Sales\"]].groupby([\"WeekOfYear\"]).agg({\"Sales\": \"mean\"})\n",
    "print(\"rows and columns: \", tsa_df_weeks.shape)\n",
    "print(DV.summ_columns(tsa_df_weeks))\n",
    "tsa_df_weeks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV.plot_line(tsa_df_weeks, \"WeekOfYear\", \"Sales\", [15, 5], \"sales per weeks\", \"sales_weeks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- The data seems like it has somewhat constant mean and SD.\n",
    "- There doesn't seem to be any seasonality as well.\n",
    "- as a result it is probably a stationary data\n",
    "- but let's verify that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using variance and mean\n",
    "def check_stationary_mv(df, title):\n",
    "    print(\"\\n\", title)\n",
    "    X = df[1:].values\n",
    "    split = round(len(X) / 2)\n",
    "    X1, X2 = X[0:split], X[split:]\n",
    "    mean1, mean2 = X1.mean(), X2.mean()\n",
    "    var1, var2 = X1.var(), X2.var()\n",
    "    print('mean1= %f, mean2= %f' % (mean1, mean2))\n",
    "    print('variance1= %f, variance2= %f' % (var1, var2))\n",
    "\n",
    "check_stationary_mv(tsa_df_days, \"sales per days\")\n",
    "check_stationary_mv(tsa_df_weeks, \"sales per weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using The Augmented Dickey-Fuller test\n",
    "# the null hypothesis is that the series is non-stationary\n",
    "# if p-value is < 0.05 we will reject the null hypothesis\n",
    "\n",
    "def check_stationary(df, col, title):\n",
    "    print(\"\\n\",title )\n",
    "    adfResult = adfuller(df[col].values, autolag='AIC')\n",
    "    print(f'ADF Statistic: {adfResult[0]}')\n",
    "    print(f'p-value: {adfResult[1]}')\n",
    "    return adfResult\n",
    "\n",
    "check_stationary(tsa_df_days, \"Sales\", \"sales per day\")\n",
    "check_stationary(tsa_df_weeks, \"Sales\", \"sales per weeks\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- the dataset is stationary because ADF is big negative and p-value is below 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrPlots(array: np.array, prefix: str):\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.title(f\"{prefix}  Autocorrelations of Sales\")\n",
    "    plt.bar(range(len(array)), array)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting autocorrelation graph for sales per day\n",
    "acf_days = acf(tsa_df_days.Sales.values, fft=True, nlags=941)\n",
    "acfNp = np.array(acf_days)\n",
    "pacf_days = pacf(tsa_df_days.Sales.values, nlags=200)\n",
    "pacfNp = np.array(pacf_days)\n",
    "\n",
    "corrPlots(acfNp, '')\n",
    "corrPlots(pacfNp, \"Paritial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting autocorrelation graph for sales per weeks\n",
    "plt.rc(\"figure\", figsize=(10,6))\n",
    "pd.plotting.autocorrelation_plot(tsa_df_weeks['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data from -1 go 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(tsa_df_days.Sales.values.reshape([-1, 1]))\n",
    "SalesScaled = scaler.transform(tsa_df_days.Sales.values.reshape(-1, 1))\n",
    "tsa_df_days['SalesScaled'] = SalesScaled\n",
    "tsa_df_days.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation set separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(tsa_df_days.SalesScaled)\n",
    "WINDOW_SIZE = 48\n",
    "BATCH_SIZE= SIZE-WINDOW_SIZE*2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateTrain = tsa_df_days.index.values[0:BATCH_SIZE]\n",
    "DateValid = tsa_df_days.index.values[BATCH_SIZE:]\n",
    "XTrain = tsa_df_days.SalesScaled.values[0:BATCH_SIZE].astype('float32')\n",
    "XValid = tsa_df_days.SalesScaled.values[BATCH_SIZE:].astype('float32')\n",
    "\n",
    "# Obtain shapes for vectors of size (,1) for dates series\n",
    "\n",
    "DateTrain = np.reshape(DateTrain, (-1, 1))\n",
    "DateValid = np.reshape(DateValid, (-1, 1))\n",
    "\n",
    "print(\"Shape of the training set date series: \", DateTrain.shape)\n",
    "print(\"Shape of the validation set date series: \", DateValid.shape)\n",
    "print()\n",
    "print(\"Shape of the training set logarithm of sales series: \", XTrain.shape)\n",
    "print(\"Shape of the validation set logarithm of sales series in a stateless LSTM: \", XValid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "# add extra dimension\n",
    "series = tf.expand_dims(XTrain, axis=-1)\n",
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor from each individual element\n",
    "dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a window_size + 1 chunk from the slices\n",
    "dataset = dataset.window(WINDOW_SIZE + 1, shift=1, drop_remainder=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Window\n",
    "datasetEx = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "datasetEx = datasetEx.window(5, shift=1, drop_remainder=True)\n",
    "for window in datasetEx:\n",
    "    print([elem.numpy() for elem in window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(WINDOW_SIZE + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE): \n",
    "  series = tf.expand_dims(series, axis=-1)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True) \n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTrain = windowed_dataset(XTrain)\n",
    "DatasetVal = windowed_dataset(XValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(8, input_shape=[None, 1], return_sequences=True))\n",
    "model.add(LSTM(4, input_shape=[None, 1]))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"huber_loss\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"LSTM modeling experiment\")\n",
    "mlflow.tensorflow.autolog()\n",
    "history = model.fit(DatasetTrain, epochs=EPOCHS, validation_data=DatasetVal, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Accuracy of Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "  ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "  ds = ds.window(window_size, shift=1, drop_remainder=True) \n",
    "  ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "  ds = ds.batch(SIZE).prefetch(1)\n",
    "  forecast = model.predict(ds)\n",
    "  return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forecast = model_forecast(model, houseSales.HouseSalesScaled.values[:, np.newaxis], WINDOW_SIZE)\n",
    "Results = Forecast[BATCH_SIZE-WINDOW_SIZE:-1]\n",
    "Results1 = scaler.inverse_transform(Results.reshape(-1,1))\n",
    "XValid1 = scaler.inverse_transform(XValid.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 8))\n",
    "plt.title(\"LSTM Model Forecast Compared to Validation Data\")\n",
    "plt.plot(DateValid.astype('datetime64'), Results1, label='Forecast series')\n",
    "plt.plot(DateValid.astype('datetime64'), np.reshape(XValid1, (2*WINDOW_SIZE, 1)), label='Validation series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Thousands of Units')\n",
    "plt.xticks(DateValid.astype('datetime64')[:,-1], rotation = 90) \n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "MAE = tf.keras.metrics.mean_absolute_error(XValid1[:,-1], Results[:,-1]).numpy()\n",
    "RMSE = np.sqrt(tf.keras.metrics.mean_squared_error(XValid1[:,-1], Results[:,-1]).numpy())\n",
    "\n",
    "textstr = \"MAE = \" + \"{:.3f}\".format(MAE) + \"  RMSE = \" + \"{:.3f}\".format(RMSE)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "plt.annotate(textstr, xy=(0.87, 0.05), xycoords='axes fraction')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = strftime(\"%Y-%m-%d-%H:%M:%S\", gmtime())\n",
    "model.save(f'../models/LSTM_sales {time}.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b997b5b8794c030e25a28be498e8226ee8897df85ede9190db777bdc9cc75be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
